from io import BytesIO
import re
import cv2
import numpy as np
from typing import List, Dict
from loguru import logger
from PIL import Image
import pytesseract
from paddleocr import PaddleOCR
import zxingcpp
import pybit7z
import uuid
import docx
import fitz
import traceback
import zipfile
import base64
from bs4 import BeautifulSoup
from spire.doc import *
from spire.doc.common import *
from .dt import Node, File, Data
from .types import Types
import torch
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
import logging
import os


os.environ["TOKENIZERS_PARALLELISM"] = "false"  
logging.getLogger("transformers").setLevel(logging.ERROR)
logging.getLogger("transformers.modeling_utils").setLevel(logging.ERROR)
logging.getLogger("transformers.configuration_utils").setLevel(logging.ERROR)
logging.getLogger("PIL.TiffImagePlugin").setLevel(logging.ERROR)

# Helper functions
def encode_binary(text: str) -> bytes:
    return text.encode('utf-8')

def decode_binary(data: bytes) -> str:
    return data.decode('utf-8')

class Extractor:
    # æ·»åŠ ç±»å˜é‡æ¥ç¼“å­˜TrOCRæ¨¡å‹å’Œå¤„ç†å™¨
    trocr_processor = None
    trocr_model = None
    gpu_available = None
    
    @staticmethod
    def _initialize_trocr():
        """åˆå§‹åŒ–OCRæ¨¡å‹å’Œå¤„ç†å™¨ï¼ˆä»…åœ¨ç¬¬ä¸€æ¬¡éœ€è¦æ—¶åŠ è½½ï¼‰"""
        if Extractor.gpu_available is None:
            Extractor.gpu_available = torch.cuda.is_available()
        
        if Extractor.gpu_available and Extractor.trocr_processor is None and Extractor.trocr_model is None:
            try:
                logger.info("Initializing GPU OCR model (first time only)...")
                Extractor.trocr_processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten")
                Extractor.trocr_model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten").to("cuda")
                logger.info("GPU OCR model initialized successfully")
                return True
            except Exception as e:
                logger.warning(f"Failed to initialize GPU OCR: {e}. Will use CPU OCR instead.")
                Extractor.gpu_available = False
                return False
        return Extractor.gpu_available and Extractor.trocr_processor is not None and Extractor.trocr_model is not None

    @staticmethod
    def extract_urls(node: Node) -> List[Node]:
        nodes = []
        text = ""
        
        try:
            if isinstance(node.content, File):
                logger.debug(f"Node[{node.id}] file {node.content.mime_type}")
                text = decode_binary(node.content.content)
            elif isinstance(node.content, Data):
                logger.debug(f"Node[{node.id}] data {node.content.type}")
                text = decode_binary(node.content.content)
            
            urls = Extractor.extract_urls_from_text(text)
            logger.debug(f"Node[{node.id}] Number of urls: {len(urls)}")
            
            for url in urls:
                t_node = Node()
                t_node.id = 0
                t_node.content = Data(type="URL", content=encode_binary(url))
                t_node.prev = node
                # ä½¿ç”¨æ–°æ–¹æ³•ç»§æ‰¿çˆ¶èŠ‚ç‚¹çš„é¡µæ•°é™åˆ¶
                t_node.inherit_limits(node)
                nodes.append(t_node)
                
        except Exception as e:
            logger.error(f"Error extracting URLs: {str(e)}")
            
        return nodes

    @staticmethod
    def extract_urls_from_text(text: str) -> List[str]:
        url_pattern = r"(https?://[^\s\"<>{}]+)"
        return re.findall(url_pattern, text)

    @staticmethod
    def extract_qrcode(node: Node) -> List[Node]:
        nodes = []
        
        try:
            if isinstance(node.content, File):
                data = node.content.content
                bytes_io = BytesIO(data)
                file_bytes = np.asarray(bytearray(bytes_io.read()), dtype=np.uint8)
                img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
                # img = cv2.imread('test.png')
                barcodes = zxingcpp.read_barcodes(img)
                for barcode in barcodes:
                    # print('Found barcode:'
                    #     f'\n Text:    "{barcode.text}"'
                    #     f'\n Format:   {barcode.format}'
                    #     f'\n Content:  {barcode.content_type}'
                    #     f'\n Position: {barcode.position}')
                
                # reader = zxingcpp.BarCodeReader()
                # image = Image.open(BytesIO(data))
                # result = reader.decode(image)
                
                # if result and result.raw:
                    t_node = Node()
                    t_node.id = 0
                    t_node.content = Data(type="QRCODE", content=encode_binary(barcode.text))
                    t_node.prev = node
                    # ä½¿ç”¨inherit_limitsæ–¹æ³•ç»§æ‰¿é™åˆ¶
                    t_node.inherit_limits(node)
                    nodes.append(t_node)
                    
            elif isinstance(node.content, Data):
                logger.debug("extract_qrcode enter Data type")
                
        except Exception as e:
            logger.error(f"Error extracting QR code: {str(e)}")
            
        return nodes

    @staticmethod
    def extract_ocr(node: Node) -> List[Node]:
        nodes = []
        
        try:
            if isinstance(node.content, File):
                data = node.content.content
                
                # æ£€æŸ¥GPUå¹¶åˆå§‹åŒ–æ¨¡å‹ï¼ˆä»…åœ¨é¦–æ¬¡è°ƒç”¨æ—¶ï¼‰
                has_gpu_ocr = Extractor._initialize_trocr()
                extracted_text = ""
                
                # Open the image
                image = Image.open(BytesIO(data))
                
                if has_gpu_ocr:
                    try:
                        pixel_values = Extractor.trocr_processor(images=image, return_tensors="pt").pixel_values.to("cuda")
                        generated_ids = Extractor.trocr_model.generate(pixel_values)
                        extracted_text = Extractor.trocr_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
                    except Exception as e:
                        logger.warning(f"Error when using GPU OCR with GPU: {e}. Falling back to CPU OCR.")
                        has_gpu_ocr = False
                
                if not has_gpu_ocr:
                    # Fall back to PaddleOCR on CPU
                    ocr = PaddleOCR(use_angle_cls=True, lang='ch', use_gpu=False, 
                                    model_dir="/root/.paddleocr", show_log=False)
                    
                    # Convert PIL Image to numpy array as PaddleOCR needs numpy array
                    image_np = np.array(image)
                    
                    # Check if the image is RGBA and convert to RGB
                    if image_np.shape[-1] == 4:
                        image_np = cv2.cvtColor(image_np, cv2.COLOR_RGBA2RGB)
                    
                    # Run OCR
                    result = ocr.ocr(image_np, cls=True)
                    
                    if result:
                        # Extract text from results
                        text_results = []
                        for line in result:
                            for item in line:
                                if len(item) >= 2:  # Make sure the result has the expected format
                                    text_results.append(item[1][0])  # item[1][0] contains the recognized text
                        
                        # Join all recognized text
                        extracted_text = '\n'.join(text_results)
                
                if extracted_text:
                    t_node = Node()
                    t_node.id = 0
                    t_node.content = Data(type="OCR", content=encode_binary(extracted_text))
                    t_node.prev = node
                    # ä½¿ç”¨inherit_limitsæ–¹æ³•ç»§æ‰¿é™åˆ¶
                    t_node.inherit_limits(node)
                    nodes.append(t_node)
                    
            elif isinstance(node.content, Data):
                logger.debug("extract_ocr enter Data type")
                
        except Exception as e:
            logger.error(f"OCR processing failed: {str(e)}")
            
        return nodes

    @staticmethod
    def extract_text_from_html(html: str) -> str:
        soup = BeautifulSoup(html, 'html.parser')
        return soup.get_text(separator=' ', strip=True)
    
    @staticmethod
    def extract_urls_from_html(html: str) -> list:
        """
        æå– HTML ä¸­çš„æ‰€æœ‰ URLï¼Œè¦†ç›–å¸¸è§æ ‡ç­¾ã€å…ƒæ•°æ®ã€é¢„åŠ è½½ã€è¡¨å•ã€æ‡’åŠ è½½ã€SVG ä»¥åŠå†…è” CSS ä¸­çš„ URLã€‚
        """
        soup = BeautifulSoup(html, 'html.parser')
        urls = set()

        # 1. åŸºç¡€æ ‡ç­¾å±æ€§
        # å„æ ‡ç­¾å¯¹åº”çš„ URL å±æ€§ï¼ˆéƒ¨åˆ†æ ‡ç­¾æœ‰å¤šä¸ª URL æ¥æºï¼‰
        tag_attr_map = {
            'a': ['href'],
            'img': ['src', 'srcset'],
            'script': ['src', 'data-main'],
            'link': ['href'],
            'iframe': ['src'],
            'video': ['src', 'poster'],
            'audio': ['src'],
            'track': ['src'],
            'form': ['action'],
            'input': ['src'],  # é’ˆå¯¹ type="image" çš„æƒ…å†µ
            'object': ['data'],
            'embed': ['src']
        }

        for tag, attrs in tag_attr_map.items():
            for element in soup.find_all(tag):
                for attr in attrs:
                    value = element.get(attr)
                    if value:
                        if attr == 'srcset':
                            # srcset å¯èƒ½å½¢å¦‚ "img1.jpg 1x, img2.jpg 2x" åˆ†å‰²åæå– URL
                            for part in value.split(','):
                                candidate = part.strip().split(' ')[0].strip()
                                if candidate:
                                    urls.add(candidate)
                        else:
                            urls.add(value.strip())

        # 2. å…ƒæ•°æ®ä¸ SEO
        for meta in soup.find_all('meta'):
            # å¼€æ”¾å›¾è°± <meta property="og:image" content="url">
            if meta.get('property', '').strip().lower() == 'og:image':
                content = meta.get('content')
                if content:
                    urls.add(content.strip())
            # é¡µé¢åˆ·æ–° <meta http-equiv="refresh" content="5;url=redirect_url">
            if meta.get('http-equiv', '').lower() == 'refresh':
                content = meta.get('content', '')
                # åŒ¹é…å½¢å¦‚ "5;url=redirect_url" çš„æ ¼å¼
                m = re.search(r'url=([^;]+)', content, flags=re.IGNORECASE)
                if m:
                    urls.add(m.group(1).strip())

        # ç½‘ç«™å›¾æ ‡ã€DNSé¢„è§£æã€é¢„åŠ è½½ç­‰ä¾é  <link> æ ‡ç­¾ï¼Œä¸Šé¢å·²ç»åœ¨ tag_attr_map ä¸­å¤„ç†

        # 3. åŠ¨æ€å†…å®¹ä¸æ‡’åŠ è½½
        # ä»»æ„å«æœ‰ data-src å±æ€§
        for element in soup.find_all(attrs={"data-src": True}):
            data_src = element.get("data-src")
            if data_src:
                urls.add(data_src.strip())

        # 4. SVG å’Œç‰¹æ®Šæ ‡ç­¾
        # <image> æ ‡ç­¾å¯èƒ½ä½¿ç”¨ xlink:href æˆ– href å±æ€§
        for image in soup.find_all('image'):
            xlink = image.get('xlink:href')
            if xlink:
                urls.add(xlink.strip())
            alternative = image.get('href')
            if alternative:
                urls.add(alternative.strip())

        # 5. CSS ä¸­çš„ URLï¼ˆå†…è”æ ·å¼ä»¥åŠ <style> æ ‡ç­¾å†…ï¼‰
        # æ­£åˆ™åŒ¹é… url(...) å½¢å¼, å¯å¤„ç†å•å¼•å·ã€åŒå¼•å·æˆ–ä¸å¸¦å¼•å·çš„æƒ…å†µ
        style_pattern = re.compile(r'url\((?:\'|"|)([^\'")]+)(?:\'|"|)\)')

        # æ£€æŸ¥æ‰€æœ‰å­˜åœ¨ style å±æ€§çš„å…ƒç´ 
        for element in soup.find_all(style=True):
            style_text = element.get('style', '')
            matches = style_pattern.findall(style_text)
            for m in matches:
                if m:
                    urls.add(m.strip())

        # æ£€æŸ¥ <style> æ ‡ç­¾å†…éƒ¨çš„ CSS å†…å®¹
        for style_tag in soup.find_all('style'):
            css_content = style_tag.string
            if css_content:
                matches = style_pattern.findall(css_content)
                for m in matches:
                    if m:
                        urls.add(m.strip())

        return list(urls)
    

    @staticmethod
    def extract_img_from_html(html: str) -> list:
        img_bytes_list = []
        soup_t = BeautifulSoup(html, 'html.parser')
        # æŸ¥æ‰¾æ‰€æœ‰çš„ img æ ‡ç­¾
        images = soup_t.find_all('img')
        
        # éå† img æ ‡ç­¾ï¼Œæ‰“å°å›¾ç‰‡ src å±æ€§
        for image in images:
            # print(f"\n3.image --> {image}\n")
            # è¿™é‡Œçš„img_srcæ˜¯ï¼šdata:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAZAAAAGQCAIAAAâ€¦â€¦
            # print(f"\n3.image --> {type(image)} -- \n")
            if not image.has_attr('src'):
                continue
            img_src = image['src']
            # print(f"\n4.img_src --> {img_src}\n")
            #self._logger().debug(f"find image resource - {img_src}")
            # è¿™ä¸ªåˆ¤æ®ä¸çŸ¥é“æ˜¯å¦æ­£ç¡®ï¼Œæœ€èµ·ç æ²¡æœ‰base64ç¼–ç çš„å°±æœ‰é—®é¢˜äº†
            if (img_src.find("base64") == -1):
                continue 
            img_arrays1 = img_src.split(';')
            
            # img_arrays1[1]ä¸­çš„æ•°æ®æ˜¯ï¼šbase64,iVBORw0KGgoAAAANSUhEUgAAAZAAAAGQCAIAAAâ€¦â€¦
            img_arrays2 = img_arrays1[1].split(',')
            if len(img_arrays2) < 1:
                continue
            if img_arrays2[0] != 'base64':
                continue
            
            # è¿™é‡Œ img_str ä¸­æ˜¯ base64 ç¼–ç çš„ï¼šiVBORw0KGgoAAAANSUhEUgAAAZAAAAGQCAIAAAâ€¦â€¦
            img_str = img_arrays2[1]
            
            # è¿™é‡Œçš„ img_bytes å°±æ˜¯å›¾ç‰‡çš„äºŒè¿›åˆ¶æ•°ç»„ï¼Œbase64 è§£ç åçš„
            img_bytes = base64.b64decode(img_str)

            img_bytes_list.append(img_bytes)

        return img_bytes_list


    @staticmethod
    def extract_html(node: Node) -> List[Node]:
        nodes = []
        text = ""
        
        try:
            if isinstance(node.content, File):
                logger.debug(f"Node[{node.id}] file {node.content.mime_type}")
                text = decode_binary(node.content.content)
            elif isinstance(node.content, Data):
                logger.debug(f"Node[{node.id}] data {node.content.type}")
                text = decode_binary(node.content.content)
            
            html_text = Extractor.extract_text_from_html(text)
            
            t_node = Node()
            t_node.id = 0
            t_node.content = Data(type="TEXT", content=encode_binary(html_text))
            t_node.prev = node
            # ç»§æ‰¿çˆ¶èŠ‚ç‚¹çš„é¡µæ•°é™åˆ¶
            t_node.pdf_max_pages = node.pdf_max_pages
            t_node.word_max_pages = node.word_max_pages
            nodes.append(t_node)

            html_urls = Extractor.extract_urls_from_html(text)
            for url in html_urls:
                    t_node = Node()
                    t_node.id = 0
                    t_node.content = Data(type="URL", content=encode_binary(url))
                    t_node.prev = node
                    # ç»§æ‰¿çˆ¶èŠ‚ç‚¹çš„é¡µæ•°é™åˆ¶
                    t_node.pdf_max_pages = node.pdf_max_pages
                    t_node.word_max_pages = node.word_max_pages
                    nodes.append(t_node)

            img_bytes_list = Extractor.extract_img_from_html(text)
            for img_bytes in img_bytes_list:
                t_node = Node()
                t_node.content = File(
                    path="",
                    name="",
                    content=img_bytes
                )
                t_node.prev = node
                # ç»§æ‰¿çˆ¶èŠ‚ç‚¹çš„é¡µæ•°é™åˆ¶
                t_node.pdf_max_pages = node.pdf_max_pages
                t_node.word_max_pages = node.word_max_pages
                nodes.append(t_node)
            
        except Exception as e:
            logger.error(f"Error extracting HTML: {str(e)}")
            
        return nodes

    @staticmethod
    def extract_compressed_file(node: Node) -> List[Node]:
        nodes = []
        data = None
        
        try:
            if isinstance(node.content, File):
                data = node.content.content
            elif isinstance(node.content, Data):
                logger.debug("extract_compressed_file enter Data type")
                return nodes
            
            extracted = False
            files = {}
            
            if not node.passwords:
                try:
                    files = Extractor.extract_files_from_data(data)
                    extracted = True
                except Exception as e:
                    # if "password is required" not in str(e).lower():
                    # éœ€è¦å¯†ç çš„æŠ¥é”™
                    # Failed to extract the archive: A password is required but none was provided.
                    raise e

            if not extracted and node.passwords:
                for password in node.passwords:
                    try:
                        files = Extractor.extract_files_from_data(data, password)
                        extracted = True
                        node.meta.map_string["correct_password"] = password
                        break
                    except Exception as e:
                        if "Wrong password" in str(e):
                            logger.error(f"Password error: {e}")
                            continue
                        raise e

            if not extracted:
                raise RuntimeError("Failed to extract compressed file")

            for filename, content in files.items():
                t_node = Node()
                t_node.content = File(
                    path=filename,
                    name=filename,
                    content=content
                )
                t_node.prev = node
                # ä½¿ç”¨inherit_limitsæ–¹æ³•ç»§æ‰¿é™åˆ¶
                t_node.inherit_limits(node)
                nodes.append(t_node)

        except Exception as e:
            logger.error(f"Error extracting compressed file: {str(e)}")
            raise e

        return nodes

    @staticmethod
    def extract_files_from_data(data: bytes, password: str = "") -> Dict[str, bytes]:
        files_map = {}
        
        try:
            with pybit7z.lib7zip_context() as lib:
                extractor = pybit7z.BitMemExtractor(lib, pybit7z.FormatAuto)
                if password:
                    extractor.set_password(password)
                files_map = extractor.extract(data)
                                
        except Exception as e:
            logger.error(f"Error in extract_files_from_data: {str(e)}")
            raise e
            
        return files_map

    @staticmethod
    def extract_word_file(node: Node) -> List[Node]:
        node.meta.map_bool["is_encrypted"] = False
        nodes = []
        file: File
        if isinstance(node.content, File):
            file = node.content
        elif isinstance(node.content, Data):
            logger.error("extract_word_file enter Data type")
            return nodes
        
        def analyze_docx_file(analyzed_path:str):
            ole_file_count = 0
            doc = docx.Document(analyzed_path)
            text_content = []
            
            # é™åˆ¶å¤„ç†çš„æ®µè½æ•°é‡ï¼Œæ ¹æ®é¡µæ•°ä¼°ç®—
            # å‡è®¾æ¯é¡µçº¦æœ‰20ä¸ªæ®µè½
            max_paragraphs = node.word_max_pages * 20
            for i, para in enumerate(doc.paragraphs):
                if i >= max_paragraphs:
                    break
                text_content.append(para.text)

            separator = "\n"
            joined_text_content = separator.join(text_content)
            # æš‚æ—¶ä¿ç•™ä¸‹é¢çš„è¯ ğŸ‘‡
            # if 'æ­¤æ–‡ä»¶ä»…WPSèƒ½æ‰“å¼€' not in text_content:
            t_node = Node()
            t_node.id = 0
            t_node.content = Data(type="TEXT", content=encode_binary(joined_text_content))
            t_node.prev = node
            # ä½¿ç”¨inherit_limitsæ–¹æ³•ç»§æ‰¿é™åˆ¶
            t_node.inherit_limits(node)
            nodes.append(t_node)

            with zipfile.ZipFile(analyzed_path, 'r') as docx_zip:
                all_files = docx_zip.namelist()
                for file1 in all_files:
                    if file1.startswith('word/embeddings/') and not file1.__eq__('word/embeddings/'):
                        ole_file_count += 1
                    elif file1.startswith('word/media/') and not file1.__eq__('word/media/'):
                        t_node = Node()
                        file_name = os.path.basename(file1)
                        file_byte = docx_zip.read(file1)
                        t_node.content = File(
                            path=file_name,
                            name=file_name,
                            content=file_byte
                        )
                        t_node.prev = node
                        # ä½¿ç”¨inherit_limitsæ–¹æ³•ç»§æ‰¿é™åˆ¶
                        t_node.inherit_limits(node)
                        nodes.append(t_node)

            if ole_file_count:
                doc = Document()
                doc.LoadFromFile(analyzed_path)

                i = 1 
                for k in range(doc.Sections.Count):
                    sec = doc.Sections.get_Item(k)
                    # Iterate through all child objects in the body of each section
                    for j in range(sec.Body.ChildObjects.Count):
                        obj = sec.Body.ChildObjects.get_Item(j)
                        # Check if the child object is a paragraph
                        if isinstance(obj, Paragraph):
                            par = obj if isinstance(obj, Paragraph) else None
                            # Iterate through the child objects in the paragraph
                            for m in range(par.ChildObjects.Count):
                                o = par.ChildObjects.get_Item(m)
                                # Check if the child object is an OLE object
                                if o.DocumentObjectType == DocumentObjectType.OleObject:
                                    ole = o if isinstance(o, DocOleObject) else None
                                    s = ole.ObjectType
                                    scanners = []
                                    # Check if the OLE object is a PDF file
                                    if s.startswith("AcroExch.Document"):
                                        ext = ".pdf"
                                        scanners.append("ScanPdf")
                                    # Check if the OLE object is an Excel spreadsheet
                                    elif s.startswith("Excel.Sheet"):
                                        ext = ".xlsx"
                                    # Check if the OLE object is a PowerPoint presentation
                                    elif s.startswith("PowerPoint.Show"):
                                        ext = ".pptx"
                                    elif s.startswith("Package"):
                                        ext = ""
                                        scanners.append("ScanCompressed")
                                    elif s.startswith("Word.Document.12"):
                                        ext = ".docx"
                                        scanners.append("ScanDocx")
                                    elif s.startswith("Word.Document.8"):
                                        ext = ".doc"
                                        scanners.append("ScanDoc")
                                    else:
                                        continue
                                    t_node = Node()
                                    t_node.content = File(
                                        path=f"Output/OLE{i}{ext}",
                                        name=f"Output/OLE{i}{ext}",
                                        content=ole.NativeData
                                    )
                                    t_node.prev = node
                                    # ä½¿ç”¨inherit_limitsæ–¹æ³•ç»§æ‰¿é™åˆ¶
                                    t_node.inherit_limits(node)
                                    nodes.append(t_node)                
                                    i += 1

                doc.Close()
        
        tmp_files = []

        current_file_path = ''
        file_path = '/tmp/' + uuid.uuid4().__str__()
        with open(file_path, 'wb') as f:
            f.write(file.content)
        tmp_files.append(file_path)
        current_file_path = file_path
        if node.type == Types.DOC:
            docx_path = file_path + 'x'
            document = Document()
            document.LoadFromFile(file_path)
            document.SaveToFile(docx_path, FileFormat.Docx)
            tmp_files.append(docx_path)
            current_file_path = docx_path

        try:
            analyze_docx_file(current_file_path)
        except zipfile.BadZipFile:
            node.meta.map_bool["is_encrypted"] = True
            stop = False
            for pwd in node.passwords:
                if stop: break
                try:
                    document = Document()
                    document.LoadFromFile(current_file_path, FileFormat.Docx, pwd)
                    document.RemoveEncryption()
                    document.SaveToFile(current_file_path)
                    stop = True
                except Exception as e:
                    pass
            if stop:
                analyze_docx_file(current_file_path)
        except Exception as e:
            traceback.print_exc()
        finally:
            for i in tmp_files:
                if os.path.exists(i):
                    os.remove(i)
        
        return nodes



            # for rel in doc.part.rels.values():
            #     if "image" in rel.reltype:
            #         image_path = rel.target_part.partname
            #         images.append(image_path)

            # print(text_content)
            # print(images)
            # if 'æ­¤æ–‡ä»¶ä»…WPSèƒ½æ‰“å¼€' not in text_content:
            #     file_name = ''
            #     if isinstance(file.original_filename, bytes):
            #         file_name = file.original_filename.decode()
            #     else:
            #         file_name = file.original_filename

            #     self.texts.append({file_name: ''.join(text_content)})
            #     extractor = URIExtractor()
            #     separator = "\n"
            #     joined_text_content = separator.join(text_content)

            #     uris:list = extractor.extracturis(' '.join(text_content))
            #     uris += extract_urls(' '.join(text_content))
            #     if set(uris):
            #         for i in uris:
            #             tmp_dict = {}
            #             tmp_dict["fileid"] = file.uid
            #             tmp_dict["string"] = i
            #             tmp_dict["flavors"] = copy.deepcopy(file.file_types)
            #             tmp_dict["from"] = "WORD-TEXT"

            #             self.urls.append(tmp_dict)
        
    @staticmethod
    def extract_pdf_file(node: Node) -> List[Node]:
        nodes = []
        file: File
        if isinstance(node.content, File):
            file = node.content
        elif isinstance(node.content, Data):
            logger.error("extract_pdf_file enter Data type")
            return nodes
        
        all_text = ""
        pdf = fitz.open(stream=BytesIO(file.content), filetype="pdf")
        if pdf.needs_pass:
            node.meta.map_bool["is_encrypted"] = True
            password_success = False
            for password in node.passwords:
                if pdf.authenticate(password):
                    password_success = True
                    node.meta.map_string["correct_password"] = password
                    break
    
            if not password_success:
                raise ValueError("PDF all passwords are invalid.")
        else:
            node.meta.map_bool["is_encrypted"] = False
            
        # ä½¿ç”¨node.pdf_max_pagesæ¥é™åˆ¶å¤„ç†çš„é¡µæ•°
        max_pages = min(node.pdf_max_pages, len(pdf))
        for page_number in range(max_pages):
            page = pdf.load_page(page_number)
            images = page.get_images(full=True)
            text = page.get_text()
            all_text += text
            # print(images)

            for img_index, img in enumerate(images):
                xref = img[0]
                base_image = pdf.extract_image(xref)
                image_bytes = base_image["image"]
                # image = Image.open(io.BytesIO(image_bytes))
                image_filename = f"page_{page_number + 1}_image_{img_index + 1}.png"
                t_node = Node()
                t_node.content = File(
                    path=image_filename,
                    name=image_filename,
                    content=image_bytes
                )
                t_node.prev = node
                # ä½¿ç”¨inherit_limitsæ–¹æ³•ç»§æ‰¿é™åˆ¶
                t_node.inherit_limits(node)
                nodes.append(t_node)

        t_node = Node()
        t_node.id = 0
        t_node.content = Data(type="TEXT", content=encode_binary(all_text))
        t_node.prev = node
        # ä½¿ç”¨inherit_limitsæ–¹æ³•ç»§æ‰¿é™åˆ¶
        t_node.inherit_limits(node)
        nodes.append(t_node)

        return nodes
